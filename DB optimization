# Use connection pooling
from sqlalchemy.pool import QueuePool

engine = create_engine(
    get_sqlalchemy_url(),
    poolclass=QueuePool,
    pool_size=5,
    max_overflow=10,
    pool_timeout=30
)

# Use chunking for large data operations
def batch_insert(df, table_name, chunk_size=1000):
    for i in range(0, len(df), chunk_size):
        chunk = df[i:i + chunk_size]
        chunk.to_sql(table_name, engine, if_exists='append', index=False)
